====================== Module 2

Demo 1

>>> Creating a cluster using gcloud

gcloud auth login

gcloud config set project spikey-gke

gcloud config set compute/zone us-central1-a

gcloud components update

gcloud container clusters list

gcloud container clusters create hello-cluster 

gcloud container clusters list


>>> configuring kubectl

> Formalities

gcloud components install kubectl


> Viewing the current context for kubectl
kubectl config current-context

> To view your environment's kubeconfig
kubectl config view

> Generating a kubeconfig entry (Setting a default cluster for kubectl commands)
gcloud container clusters get-credentials hello-cluster 

> Running individual kubectl commands against a specific cluster 
kubectl run hello-server --image gcr.io/google-samples/hello-app:1.0 --port 8080 --cluster hello-cluster
 
> Understanding kubectl commands which dont change the state of cluster
kubectl cluster-info
kubectl config view

>>> Upgrading

> Checking supported versions
gcloud container get-server-config

> Upgrading the master node 
gcloud container clusters upgrade hello-cluster --master 
gcloud container clusters upgrade hello-cluster --master --cluster-version [version]

> Upgrading nodes(It upgrde nodes to version of master node)
gcloud container clusters upgrade hello-cluster

> To install a specific release of Kubernetes
gcloud container clusters upgrade hello-cluster --cluster-version [version]


>>> Resizing a cluster 
	
> Increasing the cluster size (default-pool was having 3 nodes)
gcloud container clusters resize hello-cluster --node-pool default-pool --size 4

NOTE: When you increase the size of a node pool that spans multiple zones, the new size represents the number of nodes in the node pool per zone. For example, if you have a node pool of size 2 spanning two zones, the total node count is 4. If you resize the node pool to size 4, the total node count becomes 8.

> Decrementing the size (cuurently in beta)
gcloud beta container clusters resize hello-cluster --node-pool default-pool --size 2


>>> Auto scaling

> Enabling autoscaling for an existing node pool
gcloud container clusters update hello-cluster --enable-autoscaling --min-nodes 1 --max-nodes 10 --node-pool default-pool

> Enable auto-scaling by creating a new cluster
gcloud container clusters create second-cluster --num-nodes 6 --enable-autoscaling --min-nodes 2 --max-nodes 10


>>> Adding and Managing Node Pools

> Adding a node pool
gcloud container node-pools create second-pool --cluster hello-cluster

> Viewing node pools in a cluster
gcloud container node-pools list --cluster hello-cluster

> Resizing a node pool
gcloud container clusters resize hello-cluster --node-pool second-pool --size 2

> Upgrading a node pool
gcloud container clusters upgrade hello-cluster --node-pool second-pool

> Deleting a node pool
gcloud container node-pools delete hello-cluster --cluster second-pool

>>> Specifying a Node Image

> Upgrading to cos (Container-Optimized OS)
  since auto upgrade and auto repair are not supported on Ubuntu.
gcloud container clusters upgrade --image-type cos spikey-first- --node-pool second-pool


>>> Auto-upgrading nodes

> Enabling node auto-upgrades for an existing node pool
gcloud container node-pools update default-pool --cluster hello-cluster --enable-autoupgrade

> Disabling node auto-upgrades for an existing node pool
gcloud container node-pools update default-pool --cluster hello-cluster --no-enable-autoupgrade

>> Auto-repairing nodes

> Enabling
gcloud container node-pools update default-pool --cluster hello-cluster --enable-autorepair

> Disabling
gcloud container node-pools update default-pool --cluster hello-cluster --no-enable-autorepair


Demo 2 - gcloud


gcloud config set project spikey-gke

gcloud config set compute/region us-central1

gcloud components update

gcloud container get-server-config

gcloud container clusters create my-regional-cluster \
 --num-nodes 2 \
 --region us-central1 \
 --disk-size=15GB \
 --disk-type=pd-standard \
 --enable-autoscaling --min-nodes 1 --max-nodes 10\
 --enable-autorepair

gcloud compute machine-types list


gcloud container clusters create my-zonal-cluster --zone us-central1-a\
 --preemptible \
 --machine-type n1-standard-1 \
 --no-enable-cloud-monitoring \
 --no-enable-cloud-logging

gcloud container clusters describe my-zonal-cluster --zone us-central1-a

gcloud container clusters update my-zonal-cluster --zone us-central1-a\
 --logging-service="logging.googleapis.com" 

gcloud beta container clusters get-credentials my-regional-cluster \
 --region us-central1 \
 --project spikey-gke

gcloud container node-pools list --cluster my-regional-cluster --region us-central1

gcloud container node-pools create my-pool --num-nodes=2 \
 --cluster my-regional-cluster --region us-central1

gcloud container node-pools list --cluster my-regional-cluster --region us-central1

gcloud container clusters resize my-regional-cluster --region us-central1 \
 --node-pool my-pool --size 4 

gcloud container clusters upgrade --image-type ubuntu my-regional-cluster \
 --region us-central1 --node-pool default-pool

# ERROR Auto_upgrade and auto_repair are not supported on UBUNTU

gcloud container clusters describe my-regional-cluster --region us-central1


Demo 3 - kubectl

cat ~/.kube/config

kubectl config current-context
kubectl cluster-info 
kubectl config view

gcloud container clusters get-credentials my-zonal-cluster \
 --zone us-central1-a --project spikey-gke

kubectl config current-context
kubectl cluster-info 
kubectl config view

kubectl run nginx --image=nginx --replicas=2 

kubectl get pods -owide

kubectl get nodes

kubectl expose deployment nginx --port=80 --target-port=80 --type=LoadBalancer

kubectl get service nginx

gcloud container clusters update my-zonal-cluster \
 --zone us-central1-a \
 --node-locations us-central1-a,us-central1-b

kubectl delete service nginx

kubectl delete deployment nginx

gcloud container clusters delete my-regional-cluster --region us-central1

gcloud container clusters delete my-zonal-cluster --zone us-central1-a

======================== Module 3


Demo 1

>>> Creating cluster, docker image and deploying them using console

>> Creating docker image

cd spikeysales-image

gcloud config get-value project

export PROJECT_ID="$(gcloud config get-value project -q)"

docker build -t gcr.io/${PROJECT_ID}/spikey:v1 .

docker images

>> Uploading docker image

gcloud auth configure-docker

docker push gcr.io/${PROJECT_ID}/spikey:v1


>> Creating cluster in console

>> Deploying our image in workloads in console

>> getting ip from service in console


Demo 2

>>> Deploying an application to the cluster using kubectl directly from gcr using service

>Connecting to spikey-cluster


> Creating deployment 
kubectl run spikey-website --image gcr.io/spikey-website:v2 --port 8080 --cluster spikey-cluster

> Exposing deployment 
kubectl expose deployment spikey-website --type LoadBalancer --port 80 --target-port 8080 

> Inspecting deployed application

copy external ip from output of following command
kubectl get service spikey-website 

paste that external ip in your url
http://EXTERNAL_IP/


Demo 2

>>> Understanding volumes with deployments using menifest(yaml files)

The following Deployment manifest describes a Deployment of three Pods that each have an emptyDir Volume.

>> yaml file

apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: spikey-volume-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo
    spec:
      containers:
      - name: spikey-gke
        image: gcr.io/spikey-website:v1
        volumeMounts:
        - mountPath: /cache
          name: cache-volume
      volumes:
        - name: cache-volume
          emptyDir: {}


>> Creating a deployment from this manifest file

> Set project
gcloud config set project spikey-gke

> Set zone
gcloud config set compute/zone us-central1-a

> Connect to already created cluster (If needed to show in demo then creating cluster using UI will appear)
gcloud container clusters get-credentials spikey-cluster --zone us-central1-a --project spikey-gke 

> Creating pods by manifesting yaml files 
kubectl apply -f volumes-demo.yaml 

> Getting pods which are created
kubectl describe pods spikey-volume-deployment

>>> Understanding persistent volumes

----readonly-pv.yaml---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: spikey-readonly-pv
spec:
  storageClassName: ""
  capacity:
    storage: 10G
  accessModes:
    - ReadOnlyMany
  gcePersistentDisk:
    pdName: my-test-disk
    fsType: ext4

>> Creating a deployment from this manifest file

> Set project
gcloud config set project spikey-gke

> Set zone
gcloud config set compute/zone us-central1-a

> Connect to already created cluster (If needed to show in demo then creating cluster using UI will appear)
gcloud container clusters get-credentials spikey-cluster --zone us-central1-a --project spikey-gke 

> Creating pods by manifesting yaml files 
kubectl apply -f readonly-pv.yaml 

> Getting pods which are created
kubectl describe pods spikey-readonly-pv


---ssd-storageclass.yaml---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: spikey-faster
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-

>> Creating a deployment from this manifest file

> Set project
gcloud config set project spikey-gke

> Set zone
gcloud config set compute/zone us-central1-a

> Connect to already created cluster (If needed to show in demo then creating cluster using UI will appear)
gcloud container clusters get-credentials spikey-cluster --zone us-central1-a --project spikey-gke 

> Creating pods by manifesting yaml files 
kubectl apply -f ssd-storageclass.yaml

> Getting pods which are created
kubectl describe pods spikey-faster


Demo 3

>>> Understanding Network Policy 

https://codelabs.developers.google.com/codelabs/cloud-gke-highlights/index.html?index=..%2F..%2Fnext17#8


Demo 4

>>> Learning Network Load balancing using nginx

> Set project
gcloud config set project spikey-developers

> Set zone
gcloud config set compute/zone us-central1-c

> Set region
gcloud config set compute/region us-central1

> Checking configuration
gcloud config list 

> Creating cluster
gcloud container clusters create networklb --num-nodes 3

> Deploying nginx
kubectl run nginx --image=nginx --replicas=3

> Gettting pods
kubectl get pods -owide

> Exposing nginx on port 80 (This command will create a network load balancer to load balance traffic to the three nginx instances.)
kubectl expose deployment nginx --port=80 --target-port=80 --type=LoadBalancer

> Getting network load balancer address
kubectl get service nginx

clean up-  Undeploy nginx before moving on to deploy a full stack application.

> deleting nginx service
kubectl delete service nginx

> Delete the replication controller. This will subsequently delete the pods (all of the nginx instances) as well
kubectl delete deployment nginx

> Deleting cluster
gcloud container clusters delete networklb --num-nodes 3


Demo 5

>>> Setting up HTTP Load Balancing with Ingress

> creating cluster
gcloud container clusters create ingress-cluster --num-nodes 3


> Creating instance of nginx image running on port 80
kubectl run nginx --image=nginx --port=80

> Create a Container Engine service that exposes the nginx Pod on each Node in the cluster.
kubectl expose deployment nginx --target-port=80 --type=NodePort

> Configuration file for ingress
basic-ingress.yaml

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: basic-ingress
spec:
  backend:
    serviceName: nginx
    servicePort: 80


> Creating Ingress
> Ingress will create the HTTP load balancing resources in GCE and connect them to the deployment. 
kubectl create -f basic-ingress.yaml

> Monitoring progress
kubectl get ingress basic-ingress --watch

> Getting details
kubectl describe ingress basic-ingress

> Use this command to identify the external IP address of the load balancer.
kubectl get ingress basic-ingress

> Use curl or browse to the address to verify that nginx is being served through the load balancer.
curl 35.201.106.47


Demo 4
>> Binary Authorization

export PROJECT_ID=$(gcloud config get-value project)

mkdir binary-auth ; cd binary-auth

gcloud services enable container.googleapis.com
gcloud services enable binaryauthorization.googleapis.com


gcloud container clusters create binary-auth-cluster --zone us-central1-a

gcloud container clusters get-credentials binary-auth-cluster --zone us-central1-a




cat << EOF > Dockerfile
   FROM alpine
   CMD tail -f /dev/null
EOF


CONTAINER_PATH=us.gcr.io/$PROJECT_ID/hello-world

docker build -t $CONTAINER_PATH ./

gcloud auth configure-docker --quiet


docker push $CONTAINER_PATH


kubectl run hello-world --image $CONTAINER_PATH
kubectl get pod


gcloud beta container clusters update binary-auth-cluster \
    --enable-binauthz --zone us-central1-a


cat > ./policy.yaml << EOM
    admissionWhitelistPatterns:
    - namePattern: gcr.io/google_containers/*
    - namePattern: gcr.io/google-containers/*
    - namePattern: k8s.gcr.io/*
    defaultAdmissionRule:
      evaluationMode: ALWAYS_DENY
      enforcementMode: ENFORCED_BLOCK_AND_AUDIT_LOG
EOM


gcloud beta container binauthz policy import policy.yaml


kubectl delete deployment --all

kubectl delete event --all

kubectl run hello-world --image $CONTAINER_PATH

kubectl get pods


kubectl get event --template \
'{{range.items}}{{"\033[0;36m"}}{{.reason}}:{{"\033[0m"}}{{.message}}{{"\n"}}{{end}}'



NOTE_ID=spikey-attestor-note


cat > ./create_note_request.json << EOM
{
  "name": "projects/${PROJECT_ID}/notes/${NOTE_ID}",
  "attestation_authority": {
    "hint": {
      "human_readable_name": "This note represents an attestation authority"
    }
  }
}
EOM



curl -vvv -X POST \
    -H "Content-Type: application/json"  \
    -H "Authorization: Bearer $(gcloud auth print-access-token)"  \
    --data-binary @./create_note_request.json  \
    "https://containeranalysis.googleapis.com/v1alpha1/projects/${PROJECT_ID}/notes/?noteId=${NOTE_ID}"


curl -vvv  \
    -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    "https://containeranalysis.googleapis.com/v1alpha1/projects/${PROJECT_ID}/notes/${NOTE_ID}"




ATTESTOR_ID=spikey-binauthz-attestor

gcloud beta container binauthz attestors create $ATTESTOR_ID \
    --attestation-authority-note=$NOTE_ID \
    --attestation-authority-note-project=$PROJECT_ID


gcloud beta container binauthz attestors list


PROJECT_NUMBER=$(gcloud projects describe "${PROJECT_ID}"  --format="value(projectNumber)")

BINAUTHZ_SA_EMAIL="service-${PROJECT_NUMBER}@gcp-sa-binaryauthorization.iam.gserviceaccount.com"


cat > ./iam_request.json << EOM
{
  'resource': 'projects/$PROJECT_ID/notes/$NOTE_ID',
  'policy': {
    'bindings': [
      {
        'role': 'roles/containeranalysis.notes.occurrences.viewer',
        'members': [
          'serviceAccount:$BINAUTHZ_SA_EMAIL'
        ]
      }
    ]
  }
}
EOM


curl -X POST  \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $(gcloud auth print-access-token)" \
    --data-binary @./iam_request.json \
  "https://containeranalysis.googleapis.com/v1alpha1/projects/$PROJECT_ID/notes/$NOTE_ID:setIamPolicy"



sudo apt-get install rng-tools -y

sudo rngd -r /dev/urandom

gpg --batch --gen-key <(
    cat <<- EOF
      Key-Type: RSA
      Key-Length: 2048
      Name-Real: Demo Signing Role
      Name-Email: spikeysales@loonycorn.com
      %commit
EOF
)


gpg --armor --export spikeysales@loonycorn.com > ./public.pgp

gcloud beta container binauthz attestors public-keys add \
    --attestor=$ATTESTOR_ID  --public-key-file=./public.pgp


gcloud beta container binauthz attestors list


DIGEST=$(gcloud container images describe ${CONTAINER_PATH}:latest \
    --format='get(image_summary.digest)')


gcloud beta container binauthz create-signature-payload \
    --artifact-url="${CONTAINER_PATH}@${DIGEST}"  > ./payload.json

cat payload.json

gpg \
    --local-user spikeysales@loonycorn.com \
    --armor \
    --output ./signature.pgp \
    --sign ./payload.json


KEY_FINGERPRINT=$(gpg --list-keys spikeysales@loonycorn.com | sed -n '2p')


gcloud beta container binauthz attestations create \
   --artifact-url="${CONTAINER_PATH}@${DIGEST}" \
   --attestor=$ATTESTOR_ID \
   --attestor-project=$PROJECT_ID \
   --signature-file=./signature.pgp  \
   --pgp-key-fingerprint="$KEY_FINGERPRINT"



gcloud beta container binauthz attestations list \
   --attestor=$ATTESTOR_ID --attestor-project=$PROJECT_ID



cat << EOF > updated_policy.yaml
    admissionWhitelistPatterns:
    - namePattern: gcr.io/google_containers/*
    - namePattern: k8s.gcr.io/*
    defaultAdmissionRule:
      evaluationMode: REQUIRE_ATTESTATION
      enforcementMode: ENFORCED_BLOCK_AND_AUDIT_LOG
      requireAttestationsBy:
      - projects/$PROJECT_ID/attestors/$ATTESTOR_ID
EOF


gcloud beta container binauthz policy import updated_policy.yaml

kubectl run hello-world-signed --image "${CONTAINER_PATH}@${DIGEST}"

kubectl get pods


============== Module 4

>>> Creating jenkins 

> Clonning GCP repository 
git clone https://github.com/GoogleCloudPlatform/continuous-deployment-on-kubernetes.git


> working in that repository
cd continuous-deployment-on-kubernetes

> Creating Jenkins home volume
gcloud compute images create jenkins-home-image --source-uri https://storage.googleapis.com/solutions-public-assets/jenkins-cd/jenkins-home-v3.tar.gz

> Creating volume
gcloud compute disks create jenkins-home --image jenkins-home-image


> Configuring Jenkins Credentials
export PASSWORD=`openssl rand -base64 15`; echo "Your password is $PASSWORD"; sed -i.bak s#CHANGE_ME#$PASSWORD# jenkins/k8s/options

# Your password is 7EVHBGTX42cSBUVH8qLX

> crreating namespace
kubectl create ns jenkins

> Creating k8s secret
kubectl create secret generic jenkins --from-file=jenkins/k8s/options --namespace=jenkins

> Deploying jenkins
kubectl apply -f jenkins/k8s/

> Confirm whether pod is running or not
kubectl get pods -n jenkins

>>> Configuring HTTP Load Balancing

> Confirm that services are set up correctly 
kubectl get svc -n jenkins

> Create an SSL certificate and key.
openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout /tmp/tls.key -out /tmp/tls.crt -subj "/CN=jenkins/O=jenkins"

# writing new private key to '/tmp/tls.key'

> Upload the certificate to Kubernetes as a secret.
kubectl create secret generic tls --from-file=/tmp/tls.crt --from-file=/tmp/tls.key -n jenkins

# secret "tls" created

> Create the HTTPS load balancer using an ingress.
kubectl apply -f jenkins/k8s/lb/ingress.yaml

>>> Connecting to Jenkins

> Check the status of the load balancer's health checks.(wait untill backend gets healthy)
kubectl describe ingress jenkins --namespace jenkins

> Once your backends are healthy, you can get the Jenkins URL
echo "Jenkins URL: https://`kubectl get ingress jenkins -n jenkins -o jsonpath='{.status.loadBalancer.ingress[0].ip}'`"; echo "Your username/password:  jenkins/$PASSWORD"

Jenkins URL: https://35.190.46.55
Your username/password:  jenkins/O2OGlb5OmRwLPTJTPd4x

Goto to above url and sign with above credentials

>>> Deploying the Application

We will deploy the application into two different environments:

* Production: The live site that your users access.
* Canary: A smaller-capacity site that receives only percentage of our user traffic. We will use this environment to validate our software with live traffic before it's released to all of our users.

> Navigate to sample-app directory
cd sample-app

> Creating the Kubernetes namespace to logically isolate the deployment.
kubectl create ns production

> Creating production, Canary and Service Deployments

kubectl apply -f k8s/production -n production
kubectl apply -f k8s/canary -n production
kubectl apply -f k8s/services -n production

> Scale up the production environment frontends. By default, only one replica of the frontend is deployed. Use the kubectl scale command to ensure that we have at least 4 replicas running at all times.
kubectl scale deployment gceme-frontend-production -n production --replicas 4

> Confirm that you have 5 pods running for the frontend, 4 for production traffic and 1 for canary releases. This means that changes to our canary release will only affect 1 out of 5 (20%) of users. You should also have 2 pods for the backend, 1 for production and 1 for canary.
kubectl get pods -n production -l app=gceme -l role=frontend
kubectl get pods -n production -l app=gceme -l role=backend

> Retrieve the external IP for the production services.(It can take several minutes before you see the load balancer external IP address.)
kubectl get service gceme-frontend -n production

# External IP - 35.238.237.44

> Store the frontend service load balancer IP in an environment variable for use later.
export FRONTEND_SERVICE_IP=$(kubectl get -o jsonpath="{.status.loadBalancer.ingress[0].ip}"  --namespace=production services gceme-frontend)

> Check the version output of the service by hitting the /version path. It should read 1.0.0.
curl http://$FRONTEND_SERVICE_IP/version

# 1.0.0

>>>  Creating the Jenkins Pipeline

(Billed)
> Creating a repository to host sample-app
gcloud source repos create default

git init

git config credential.helper gcloud.sh

















































































































